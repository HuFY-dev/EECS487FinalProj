{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ebab60a-550d-47a3-a728-1f12a1634705",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hufy/.conda/envs/eecs487/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "[nltk_data] Downloading package punkt to /home/hufy/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForSeq2SeqLM, \n",
    "    DistilBertModel, \n",
    "    Seq2SeqTrainer, \n",
    "    Seq2SeqTrainingArguments, \n",
    "    LlamaTokenizerFast,\n",
    "    LlamaForCausalLM,\n",
    "    AutoModelForCausalLM,\n",
    "    AutoConfig,\n",
    ")\n",
    "from accelerate import init_empty_weights, load_checkpoint_and_dispatch\n",
    "from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model\n",
    "from datasets import load_dataset, load_dataset, load_metric, Dataset\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from tqdm import tqdm\n",
    "from logging import getLogger\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize\n",
    "nltk.download('punkt')\n",
    "from rouge import Rouge\n",
    "from rouge import FilesRouge\n",
    "import pandas as pd\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e1151425-dcdb-4db0-a1c9-066db2a5c2df",
   "metadata": {},
   "outputs": [],
   "source": [
    "llama_path = \"/scratch/chaijy_root/chaijy2/hufy/.cache/huggingface/hub/LLaMA-2-hf\"\n",
    "local_only = True\n",
    "\n",
    "llama_tokenizer = LlamaTokenizerFast.from_pretrained(\n",
    "    llama_path,\n",
    "    local_files_only=local_only,\n",
    ")\n",
    "llama_config = AutoConfig.from_pretrained(llama_path)\n",
    "with init_empty_weights():\n",
    "    empty_model = AutoModelForCausalLM.from_config(llama_config)\n",
    "empty_model.tie_weights()\n",
    "bnb_quantization_config = BnbQuantizationConfig(\n",
    "    load_in_4bit=True, \n",
    "    bnb_4bit_compute_dtype=torch.bfloat16, \n",
    "    bnb_4bit_use_double_quant=True, \n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    ")\n",
    "llama_model = load_and_quantize_model(\n",
    "    empty_model, \n",
    "    weights_location=llama_path, \n",
    "    bnb_quantization_config=bnb_quantization_config, \n",
    "    device_map = \"auto\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b1da40c0-bb8b-4853-9139-eb31e3c70724",
   "metadata": {},
   "outputs": [],
   "source": [
    "DEBUG = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "96f900f1-7436-441b-bc03-6cfa67a4f07d",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8\n",
    "llama_tokenizer.padding_side = 'left'\n",
    "llama_tokenizer.pad_token = llama_tokenizer.unk_token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "66d908f2-4cd4-4193-9d48-dddeef16c2fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"squad_v2\")\n",
    "train_dataset = dataset['train']\n",
    "val_dataset = dataset['validation']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c45122e2-e523-46cb-b59f-9b66a2088783",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame(val_dataset)\n",
    "grouped_df = df.groupby(\"context\").agg(list)\n",
    "grouped_df.reset_index(inplace=True)\n",
    "# print(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "503951c8-227c-4018-ab4b-09c7cfcd7a5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hufy/.local/lib/python3.10/site-packages/pyarrow/pandas_compat.py:373: FutureWarning: is_sparse is deprecated and will be removed in a future version. Check `isinstance(dtype, pd.SparseDtype)` instead.\n",
      "  if _pandas_api.is_sparse(col):\n"
     ]
    }
   ],
   "source": [
    "val_dataset_grouped = Dataset.from_pandas(grouped_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "77910923-0d1c-40f0-9980-9423cf738e7c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1204"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(val_dataset_grouped)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "1b2e731d-2380-4a8e-9999-bf3c446667a7",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'context': '\"Southern California\" is not a formal geographic designation, and definitions of what constitutes southern California vary. Geographically, California\\'s north-south midway point lies at exactly 37° 9\\' 58.23\" latitude, around 11 miles (18 km) south of San Jose; however, this does not coincide with popular use of the term. When the state is divided into two areas (northern and southern California), the term \"southern California\" usually refers to the ten southern-most counties of the state. This definition coincides neatly with the county lines at 35° 47′ 28″ north latitude, which form the northern borders of San Luis Obispo, Kern, and San Bernardino counties. Another definition for southern California uses Point Conception and the Tehachapi Mountains as the northern boundary.',\n",
       " 'id': ['5705edcd52bb8914006896ca',\n",
       "  '5705edcd52bb8914006896cb',\n",
       "  '5705edcd52bb8914006896cc',\n",
       "  '5705edcd52bb8914006896cd',\n",
       "  '5705edcd52bb8914006896ce',\n",
       "  '5ad0297e77cf76001a686c3a',\n",
       "  '5ad0297e77cf76001a686c3b',\n",
       "  '5ad0297e77cf76001a686c3c',\n",
       "  '5ad0297e77cf76001a686c3d'],\n",
       " 'title': ['Southern_California',\n",
       "  'Southern_California',\n",
       "  'Southern_California',\n",
       "  'Southern_California',\n",
       "  'Southern_California',\n",
       "  'Southern_California',\n",
       "  'Southern_California',\n",
       "  'Southern_California',\n",
       "  'Southern_California'],\n",
       " 'question': [\"Geographically speaking, where is California's north - south midway point in terms of latitude?\",\n",
       "  'How many miles south of San Jose is the north - south midway point located?',\n",
       "  'The term \"southern\" California usually refers to how many of the southern-most counties of the state?',\n",
       "  'Other than Point Conception, what landmark is used in the other definition of southern California?',\n",
       "  'Point Conception is an example of a landmark among what boundary of southern California?',\n",
       "  'What lies at 37° 8\\' 59.23\" latitude?',\n",
       "  'What is around 18 miles south of San Jose?',\n",
       "  'What lies at 35° 48′ 27″ north latitude?',\n",
       "  'What uses Point Tehachapi and the Conception Mountains as the northern boundary?'],\n",
       " 'answers': [{'answer_start': [194, 194, 194],\n",
       "   'text': ['37° 9\\' 58.23\"', '37° 9\\' 58.23\"', '37° 9\\' 58.23\"']},\n",
       "  {'answer_start': [225, 225, 225], 'text': ['11', '11', '11']},\n",
       "  {'answer_start': [453, 453, 453], 'text': ['ten', 'ten', 'ten']},\n",
       "  {'answer_start': [740, 740],\n",
       "   'text': ['Tehachapi Mountains', 'Tehachapi Mountains']},\n",
       "  {'answer_start': [767, 736, 767],\n",
       "   'text': ['northern', 'the Tehachapi Mountains', 'northern']},\n",
       "  {'answer_start': [], 'text': []},\n",
       "  {'answer_start': [], 'text': []},\n",
       "  {'answer_start': [], 'text': []},\n",
       "  {'answer_start': [], 'text': []}]}"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_dataset_grouped[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "330ff31a-47b1-426c-82b6-7734da4b00f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "if DEBUG:\n",
    "    train_dataset = train_dataset.select(range(250))\n",
    "    val_dataset = val_dataset.select(range(25))\n",
    "    val_dataset_grouped = val_dataset_grouped.select(range(25))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4708fef0-e547-4c6e-8999-288e3341b124",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_1 = \"Context:\\nA self-described \\\"modern-day feminist\\\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.\\n\"\n",
    "summary_1 = \"Summary:\\nBeyoncé, a modern-day feminist and acclaimed entertainer, has achieved remarkable success as a best-selling artist with numerous awards, including 20 Grammys, and is recognized for her empowering themes, dynamic performances, and significant influence in music and beyond.\\n\"\n",
    "quiz_1 = \"Question:\\nHow many Grammy awards has Beyoncé won?\\n\"\n",
    "icl_example_1 = context_1 + summary_1 + quiz_1 + \"\\n\"\n",
    "control_example_1 = context_1 + quiz_1 + \"\\n\"\n",
    "\n",
    "context_2 = \"Context:\\nOn 16 March 1934, President Franklin D. Roosevelt signed the Migratory Bird Hunting Stamp Act, which requires an annual stamp purchase by all hunters over the age of sixteen. The stamps are created on behalf of the program by the US Postal Service and depict wildlife artwork chosen through an annual contest. They play an important role in habitat conservation because ninety-eight percent of all funds generated by their sale go directly toward the purchase or lease of wetland habitat for protection in the National Wildlife Refuge System.[citation needed] In addition to waterfowl, it is estimated that one third of the nation's endangered species seek food and shelter in areas protected using Duck Stamp funds.[citation needed]\\n\"\n",
    "summary_2 = \"Summary:\\nThe 1934 Migratory Bird Hunting Stamp Act, signed by President Roosevelt, requires hunters to buy annual stamps, with proceeds largely funding wildlife habitat conservation and benefiting waterfowl and endangered species.\\n\"\n",
    "quiz_2 = \"Question:\\nWhat act was signed in 1934?\\n\"\n",
    "icl_example_2 = context_2 + summary_2 + quiz_2 + \"\\n\"\n",
    "control_example_2 = context_2 + quiz_2 + \"\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "16d52cef-bca2-4fb1-9443-7a55df4c8247",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_batched_prompt(batch):\n",
    "    text = [data[\"context\"] for data in batch]\n",
    "    prompts = [icl_example_1 + icl_example_2 + \"Context:\\n\" + t + \"\\n\" for t in text]\n",
    "    inputs = llama_tokenizer(\n",
    "        prompts,\n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    return {\n",
    "        \"text\": text,\n",
    "        \"prompts\": prompts,\n",
    "        \"questions\": [data[\"question\"] for data in batch], # list[list[str]] | list[str]\n",
    "        **inputs\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "b8df500c-f307-42e9-a8ed-d816ed97304e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=False, collate_fn=generate_batched_prompt)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, collate_fn=generate_batched_prompt)\n",
    "val_grouped_dataloader = DataLoader(val_dataset_grouped, batch_size=batch_size, shuffle=False, collate_fn=generate_batched_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1ee6938e-c870-4561-9bdf-4051780998dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "str_ = \"Summary:\\nBeyoncé, a modern-day feminist and acclaimed entertainer, has achieved remarkable success as a best-selling artist with numerous awards, including 20 Grammys, and is recognized for her empowering themes, dynamic performances, and significant influence in music and beyond.\\nQuestion:\\nHow many Grammy awards has Beyoncé won?\\n\\n\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cf7c6029-a066-4e34-95fc-0c6074dbbe84",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'How many Grammy awards has Beyoncé won?'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "re.search(\"Question:\\n(.+)\\n.*\", str_).groups()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "0666e4ad-eeac-4d9f-813e-ba05f91f3830",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary...\n",
      "Number of input tokens: 999\n",
      "Prompt:\n",
      "Context:\n",
      "A self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.\n",
      "Summary:\n",
      "Beyoncé, a modern-day feminist and acclaimed entertainer, has achieved remarkable success as a best-selling artist with numerous awards, including 20 Grammys, and is recognized for her empowering themes, dynamic performances, and significant influence in music and beyond.\n",
      "Question:\n",
      "How many Grammy awards has Beyoncé won?\n",
      "\n",
      "Context:\n",
      "On 16 March 1934, President Franklin D. Roosevelt signed the Migratory Bird Hunting Stamp Act, which requires an annual stamp purchase by all hunters over the age of sixteen. The stamps are created on behalf of the program by the US Postal Service and depict wildlife artwork chosen through an annual contest. They play an important role in habitat conservation because ninety-eight percent of all funds generated by their sale go directly toward the purchase or lease of wetland habitat for protection in the National Wildlife Refuge System.[citation needed] In addition to waterfowl, it is estimated that one third of the nation's endangered species seek food and shelter in areas protected using Duck Stamp funds.[citation needed]\n",
      "Summary:\n",
      "The 1934 Migratory Bird Hunting Stamp Act, signed by President Roosevelt, requires hunters to buy annual stamps, with proceeds largely funding wildlife habitat conservation and benefiting waterfowl and endangered species.\n",
      "Question:\n",
      "What act was signed in 1934?\n",
      "\n",
      "Context:\n",
      "\"Southern California\" is not a formal geographic designation, and definitions of what constitutes southern California vary. Geographically, California's north-south midway point lies at exactly 37° 9' 58.23\" latitude, around 11 miles (18 km) south of San Jose; however, this does not coincide with popular use of the term. When the state is divided into two areas (northern and southern California), the term \"southern California\" usually refers to the ten southern-most counties of the state. This definition coincides neatly with the county lines at 35° 47′ 28″ north latitude, which form the northern borders of San Luis Obispo, Kern, and San Bernardino counties. Another definition for southern California uses Point Conception and the Tehachapi Mountains as the northern boundary.\n",
      "\n",
      "\n",
      "Generated:\n",
      "Summary:\n",
      "Southern California is a geographic designation that varies in definition. When the state is divided into two areas (northern and southern California), the term \"southern California\" usually refers to the ten southern-most counties of the state.\n",
      "Question:\n",
      "What is the northern border of southern California?\n",
      "\n",
      "Context:\n",
      "The 1934–35 Chicago Black Hawks season was the 10th season of the Chicago Black Hawks in the National Hockey League (NHL). The Black Hawks finished in fourth place in the NHL's American Division with a\n",
      "\n",
      "Generated question:\n",
      "What is the northern border of southern California?\n",
      "\n",
      "Ground truth:\n",
      "[\"Geographically speaking, where is California's north - south midway point in terms of latitude?\", 'How many miles south of San Jose is the north - south midway point located?', 'The term \"southern\" California usually refers to how many of the southern-most counties of the state?', 'Other than Point Conception, what landmark is used in the other definition of southern California?', 'Point Conception is an example of a landmark among what boundary of southern California?', 'What lies at 37° 8\\' 59.23\" latitude?', 'What is around 18 miles south of San Jose?', 'What lies at 35° 48′ 27″ north latitude?', 'What uses Point Tehachapi and the Conception Mountains as the northern boundary?']\n",
      "{'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_fmeasure': 0.0}\n",
      "{'rouge2_precision': 0.1429, 'rouge2_recall': 0.0769, 'rouge2_fmeasure': 0.1}\n",
      "{'rouge2_precision': 0.1429, 'rouge2_recall': 0.0625, 'rouge2_fmeasure': 0.087}\n",
      "{'rouge2_precision': 0.2857, 'rouge2_recall': 0.1429, 'rouge2_fmeasure': 0.1905}\n",
      "{'rouge2_precision': 0.2857, 'rouge2_recall': 0.1538, 'rouge2_fmeasure': 0.2}\n",
      "{'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_fmeasure': 0.0}\n",
      "{'rouge2_precision': 0.1429, 'rouge2_recall': 0.125, 'rouge2_fmeasure': 0.1333}\n",
      "{'rouge2_precision': 0.0, 'rouge2_recall': 0.0, 'rouge2_fmeasure': 0.0}\n",
      "{'rouge2_precision': 0.1429, 'rouge2_recall': 0.0909, 'rouge2_fmeasure': 0.1111}\n"
     ]
    }
   ],
   "source": [
    "for dp in val_grouped_dataloader:\n",
    "    print(\"Generating summary...\")\n",
    "    print(f'Number of input tokens: {len(dp[\"input_ids\"][0])}')\n",
    "    print(\"Prompt:\")\n",
    "    print(dp[\"prompts\"][0] + \"\\n\")\n",
    "    generated = llama_model.generate(\n",
    "        input_ids=dp[\"input_ids\"].to(llama_model.device), \n",
    "        attention_mask=dp[\"attention_mask\"].to(llama_model.device),\n",
    "        max_new_tokens=128,\n",
    "    )\n",
    "    results = llama_tokenizer.batch_decode(generated[:, len(dp[\"input_ids\"][0]):])\n",
    "    print(\"Generated:\")\n",
    "    print(results[0] + \"\\n\")\n",
    "    \n",
    "    questions = [re.search(\"Summary:\\n(.+)\\nQuestion:\\n(.+)\", r).groups()[1] if re.search(\"Summary:\\n(.+)\\nQuestion:\\n(.+)\", r) is not None else None for r in results]\n",
    "    print(\"Generated question:\")\n",
    "    print(questions[0] + \"\\n\")\n",
    "    print(\"Ground truth:\")\n",
    "    print(dp[\"questions\"][0])\n",
    "    \n",
    "    for pred, trues in zip(questions, dp[\"questions\"]):\n",
    "        for true in trues:\n",
    "            score = compute_metrics([pred], [true])\n",
    "            print(score)\n",
    "        break\n",
    "    # summary = [result.split(\"\\n\")[0] + \"\\n\" for result in results]\n",
    "    # print(summary)\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "dd706dd0-c9b6-428c-b62a-9f41ca7f5e3b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_610590/4132584981.py:1: FutureWarning: load_metric is deprecated and will be removed in the next major version of datasets. Use 'evaluate.load' instead, from the new library 🤗 Evaluate: https://huggingface.co/docs/evaluate\n",
      "  rouge = load_metric(\"rouge\")\n"
     ]
    }
   ],
   "source": [
    "rouge = load_metric(\"rouge\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "edf9fa3b-61a9-49cb-b612-f2d8d84d238c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred: list[str], labels: list[str]):\n",
    "    rouge_output = rouge.compute(\n",
    "        predictions=pred, references=labels, rouge_types=[\"rouge2\"]\n",
    "    )[\"rouge2\"].mid\n",
    "\n",
    "    return {\n",
    "        \"rouge2_precision\": round(rouge_output.precision, 4),\n",
    "        \"rouge2_recall\": round(rouge_output.recall, 4),\n",
    "        \"rouge2_fmeasure\": round(rouge_output.fmeasure, 4),\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeac7e4a-3168-4fa4-aaa0-3489cbdca8be",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "29d224e9-571f-42cf-b18e-540a38af4b71",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating summary...\n",
      "['The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n']\n",
      "Generating quiz...\n",
      "['The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n', 'The Normans were a people descended from Norse raiders and pirates from Denmark, Iceland and Norway who, under their leader Rollo, agreed to swear fealty to King Charles III of West Francia. Through generations of assimilation and mixing with the native Frankish and Roman-G\\n']\n"
     ]
    }
   ],
   "source": [
    "for dp in val_dataloader:\n",
    "    print(\"Generating summary...\")\n",
    "    generated = llama_model.generate(\n",
    "        input_ids=dp[\"input_ids\"].to(llama_model.device), \n",
    "        attention_mask=dp[\"attention_mask\"].to(llama_model.device),\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "    results = llama_tokenizer.batch_decode(generated[:, len(dp[\"input_ids\"][0]):])\n",
    "    # print(\"Original:\")\n",
    "    # print(dp[\"text\"][0] + \"\\n\")\n",
    "    # print(\"Generated:\")\n",
    "    # print(results[0].split(\"\\n\")[0] + \"\\n\")\n",
    "    summary = [result.split(\"\\n\")[0] + \"\\n\" for result in results]\n",
    "    print(summary)\n",
    "    print(\"Generating quiz...\")\n",
    "    prompts = [\"Context:\\n\" + t + \"One quiz on the context:\\n\" for t in summary]\n",
    "    inputs = llama_tokenizer(\n",
    "        prompts,\n",
    "        padding=True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    quiz = llama_model.generate(\n",
    "        input_ids=inputs[\"input_ids\"].to(llama_model.device), \n",
    "        attention_mask=inputs[\"attention_mask\"].to(llama_model.device),\n",
    "        max_new_tokens=64,\n",
    "    )\n",
    "    quizzes = llama_tokenizer.batch_decode(quiz[:, len(inputs[\"input_ids\"][0]):])\n",
    "    quizzes = [q.split(\"\\n\")[0] + \"\\n\" for q in quizzes]\n",
    "    print(quizzes)\n",
    "    break"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "eecs487",
   "language": "python",
   "name": "eecs487"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
