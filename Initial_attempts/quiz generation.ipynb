{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install spacy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Base function for short summarization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_question_generator(text):\n",
    "    # Splitting the text into sentences\n",
    "    sentences = text.split('. ')\n",
    "    questions = []\n",
    "    for sentence in sentences:\n",
    "        if \"will\" in sentence:\n",
    "            question = sentence.replace(\"will\", \"Who will\", 1) + \"?\"\n",
    "        elif \"is\" in sentence:\n",
    "            question = sentence.replace(\"is\", \"What is\", 1) + \"?\"\n",
    "        elif \"are\" in sentence:\n",
    "            question = sentence.replace(\"are\", \"What are\", 1) + \"?\"\n",
    "        else:\n",
    "            words = sentence.split()\n",
    "            if len(words) > 0:\n",
    "                question = \"What \" + ' '.join(words) + \"?\"\n",
    "            questions.append(question)\n",
    "\n",
    "    return questions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "What It provides the necessary heat and light for life on Earth?\n",
      "What Planets orbit around the sun in predictable paths?\n"
     ]
    }
   ],
   "source": [
    "# Shortened summary\n",
    "summary = \"\"\"\n",
    "The sun is a star located at the center of our solar system. It provides the necessary heat and light for life on Earth. Planets orbit around the sun in predictable paths. Scientists are constantly studying the sun to understand its impact on Earth's climate. Future missions will explore the possibility of harnessing solar energy more effectively. The sun's surface is extremely hot, and its core generates energy through nuclear fusion. Understanding solar phenomena like solar flares and sunspots are crucial for space weather forecasting.\n",
    "\"\"\"\n",
    "\n",
    "# Generate questions\n",
    "generated_questions = simple_question_generator(summary)\n",
    "for question in generated_questions:\n",
    "    print(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic fill-in-the-blank quiz generation using nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (3.8.1)\n",
      "Requirement already satisfied: click in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (2023.10.3)\n",
      "Requirement already satisfied: tqdm in /Library/Frameworks/Python.framework/Versions/3.11/lib/python3.11/site-packages (from nltk) (4.66.1)\n",
      "Question: ____ is the capital of France . The Eiffel Tower is located there .\n",
      "Answer: Paris\n",
      "\n",
      "Question: Paris is the ____ of France . The Eiffel Tower is located there .\n",
      "Answer: capital\n",
      "\n",
      "Question: Paris is the capital of ____ . The Eiffel Tower is located there .\n",
      "Answer: France\n",
      "\n",
      "Question: Paris is the capital of France . The ____ Tower is located there .\n",
      "Answer: Eiffel\n",
      "\n",
      "Question: Paris is the capital of France . The Eiffel ____ is located there .\n",
      "Answer: Tower\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/songzhixiao/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/songzhixiao/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "#!pip install nltk\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "def generate_fill_in_the_blank_quiz(text):\n",
    "    word_tokens = word_tokenize(text)\n",
    "    pos_tags = pos_tag(word_tokens)\n",
    "\n",
    "    questions = []\n",
    "    for i, (word, tag) in enumerate(pos_tags):\n",
    "        if tag in ['NN', 'NNS', 'NNP', 'NNPS']:  # Nouns\n",
    "            question = ' '.join(word_tokens[:i] + ['____'] + word_tokens[i + 1:])\n",
    "            questions.append((question, word))\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "text = \"Paris is the capital of France. The Eiffel Tower is located there.\"\n",
    "quiz = generate_fill_in_the_blank_quiz(text)\n",
    "for question, answer in quiz:\n",
    "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "True / False question using scikit-learn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Question: True or False: The capital of Italy is Rome.\n",
      "Answer: True\n",
      "\n",
      "Question: True or False: The Eiffel Tower is in Paris.\n",
      "Answer: False\n",
      "\n",
      "Question: True or False: The Nile is the longest river in the world.\n",
      "Answer: True\n",
      "\n",
      "Question: True or False: The capital of Italy is Rome.\n",
      "Answer: False\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#!pip install scikit-learn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import random\n",
    "\n",
    "def generate_true_false_quiz(sentences):\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(sentences)\n",
    "\n",
    "    questions = []\n",
    "    for i, sentence in enumerate(sentences):\n",
    "        # Generate a false sentence\n",
    "        false_sentence = random.choice(sentences)\n",
    "        \n",
    "        # Check similarity to avoid very similar sentences\n",
    "        similarity = cosine_similarity(tfidf_matrix[i], vectorizer.transform([false_sentence]))\n",
    "        if similarity < 0.5:\n",
    "            questions.append((f\"True or False: {sentence}\", \"True\"))\n",
    "            questions.append((f\"True or False: {false_sentence}\", \"False\"))\n",
    "\n",
    "    return questions\n",
    "\n",
    "# Example usage\n",
    "sentences = [\n",
    "    \"The Eiffel Tower is in Paris.\",\n",
    "    \"The capital of Italy is Rome.\",\n",
    "    \"The Nile is the longest river in the world.\"\n",
    "]\n",
    "quiz = generate_true_false_quiz(sentences)\n",
    "for question, answer in quiz:\n",
    "    print(f\"Question: {question}\\nAnswer: {answer}\\n\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ECR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Who or what is Muhammadu Buhari?\n",
      "Who or what is Nigeria?\n",
      "Who or what is Boko Haram?\n",
      "Who or what is Chad?\n",
      "Who or what is Cameroon?\n",
      "Who or what is Niger?\n"
     ]
    }
   ],
   "source": [
    "#ECR\n",
    "import spacy\n",
    "import random\n",
    "\n",
    "# Load SpaCy model for NER\n",
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "def extract_entities(text):\n",
    "    doc = nlp(text)\n",
    "    entities = [(entity.text, entity.label_) for entity in doc.ents]\n",
    "    return entities\n",
    "\n",
    "def generate_questions_from_entities(text, entities):\n",
    "    questions = []\n",
    "    for entity, label in entities:\n",
    "        if label in ['PERSON', 'ORG', 'GPE', 'LOC']:  # Focusing on specific entity types\n",
    "            question = f\"Who or what is {entity}?\"\n",
    "            questions.append(question)\n",
    "        elif label in ['DATE', 'TIME']:\n",
    "            question = f\"When did {entity} occur?\"\n",
    "            questions.append(question)\n",
    "        elif label in ['NORP', 'EVENT']:\n",
    "            question = f\"What can you tell about {entity}?\"\n",
    "            questions.append(question)\n",
    "    return questions\n",
    "\n",
    "# Example text\n",
    "text = \"Muhammadu Buhari plans to fight corruption in Nigeria and address the nation's unrest. He'll focus on violence in the northeast, where Boko Haram operates, and cooperate with Chad, Cameroon, and Niger.\"\n",
    "\n",
    "# Extract entities and generate questions\n",
    "entities = extract_entities(text)\n",
    "questions = generate_questions_from_entities(text, entities)\n",
    "\n",
    "for question in questions:\n",
    "    print(question)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seq2Seq Model for Question Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding\n",
    "import numpy as np\n",
    "\n",
    "# Sample data (replace with actual data)\n",
    "paragraphs = [\n",
    "    'The Eiffel Tower, located on the Champ de Mars in Paris, is a wrought-iron lattice tower named after the engineer Gustave Eiffel. It was constructed from 1887 to 1889 as the entrance to the 1889 World’s Fair.',\n",
    "    'The Sahara is the largest hot desert in the world, covering large parts of North Africa. It is known for its harsh environment and extreme temperatures during the day and night.'\n",
    "]\n",
    "questions = ['Where is the Eiffel Tower?', 'What is the Sahara?']\n",
    "\n",
    "# Tokenization\n",
    "tokenizer = Tokenizer()\n",
    "tokenizer.fit_on_texts(paragraphs + questions)\n",
    "vocab_size = len(tokenizer.word_index) + 1\n",
    "\n",
    "# Convert text to sequences\n",
    "seq_paragraphs = tokenizer.texts_to_sequences(paragraphs)\n",
    "seq_questions = tokenizer.texts_to_sequences(questions)\n",
    "\n",
    "# Padding\n",
    "max_paragraph_length = max(len(seq) for seq in seq_paragraphs)\n",
    "max_question_length = max(len(seq) for seq in seq_questions)\n",
    "seq_paragraphs = pad_sequences(seq_paragraphs, maxlen=max_paragraph_length, padding='post')\n",
    "seq_questions = pad_sequences(seq_questions, maxlen=max_question_length, padding='post')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_dim = 256  # Latent dimensionality\n",
    "\n",
    "# Encoder\n",
    "encoder_inputs = Input(shape=(None,))\n",
    "enc_emb = Embedding(vocab_size, latent_dim)(encoder_inputs)\n",
    "encoder_lstm = LSTM(latent_dim, return_state=True)\n",
    "encoder_outputs, state_h, state_c = encoder_lstm(enc_emb)\n",
    "encoder_states = [state_h, state_c]\n",
    "\n",
    "# Decoder\n",
    "decoder_inputs = Input(shape=(None,))\n",
    "dec_emb_layer = Embedding(vocab_size, latent_dim)\n",
    "dec_emb = dec_emb_layer(decoder_inputs)\n",
    "decoder_lstm = LSTM(latent_dim, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(dec_emb, initial_state=encoder_states)\n",
    "decoder_dense = Dense(vocab_size, activation='softmax')\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "\n",
    "# Define the model\n",
    "model = Model([encoder_inputs, decoder_inputs], decoder_outputs)\n",
    "\n",
    "# Compile the model\n",
    "model.compile(optimizer='rmsprop', loss='categorical_crossentropy')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encoder Inference Model\n",
    "encoder_model = Model(encoder_inputs, encoder_states)\n",
    "\n",
    "# Decoder Inference Model\n",
    "decoder_state_input_h = Input(shape=(latent_dim,))\n",
    "decoder_state_input_c = Input(shape=(latent_dim,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "dec_emb2 = dec_emb_layer(decoder_inputs)\n",
    "\n",
    "decoder_outputs2, state_h2, state_c2 = decoder_lstm(dec_emb2, initial_state=decoder_states_inputs)\n",
    "decoder_states2 = [state_h2, state_c2]\n",
    "decoder_outputs2 = decoder_dense(decoder_outputs2)\n",
    "\n",
    "decoder_model = Model(\n",
    "    [decoder_inputs] + decoder_states_inputs,\n",
    "    [decoder_outputs2] + decoder_states2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors\n",
    "    states_value = encoder_model.predict(input_seq)\n",
    "\n",
    "    # Generate empty target sequence of length 1\n",
    "    target_seq = np.zeros((1, 1))\n",
    "    # Populate the first word of target sequence with the start token\n",
    "    target_seq[0, 0] = tokenizer.word_index['start']  # Assuming 'start' is the start token\n",
    "\n",
    "    stop_condition = False\n",
    "    decoded_sentence = ''\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict([target_seq] + states_value)\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_word = tokenizer.index_word[sampled_token_index]\n",
    "        decoded_sentence += ' ' + sampled_word\n",
    "\n",
    "        # Exit condition: either hit max length or find stop token\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
