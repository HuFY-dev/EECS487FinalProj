from transformers import (
    LlamaTokenizerFast,
    LlamaForCausalLM,
    AutoModelForCausalLM,
    AutoConfig,
)
from accelerate import init_empty_weights, load_checkpoint_and_dispatch
from accelerate.utils import BnbQuantizationConfig, load_and_quantize_model
from datasets import load_dataset, load_dataset, load_metric, Dataset
import torch
from torch.utils.data import DataLoader
from tqdm import tqdm
from rouge import Rouge
import pandas as pd
import re
from argparse import ArgumentParser
import wandb

parser = ArgumentParser()
parser.add_argument('--debug', action="store_true")
parser.add_argument('--control', action="store_true")
parser.add_argument('--model_path', default="/scratch/chaijy_root/chaijy2/hufy/.cache/huggingface/hub/LLaMA-2-hf", required=False)
parser.add_argument('--batch_size', type=int, default=8, required=False)
parser.add_argument('--num_shots', type=int, default=2, choices=[1,2], required=False)
args = parser.parse_args()

DEBUG = args.debug
CONTROL = args.control
BATCH_SIZE = args.batch_size
NUM_SHOTS = args.num_shots

# Load LLaMA-2 with 4 bit quantization
llama_path = args.model_path
local_only = True
llama_tokenizer = LlamaTokenizerFast.from_pretrained(
    llama_path,
    local_files_only=local_only,
)
llama_config = AutoConfig.from_pretrained(llama_path)
with init_empty_weights():
    empty_model = AutoModelForCausalLM.from_config(llama_config)
empty_model.tie_weights()
bnb_quantization_config = BnbQuantizationConfig(
    load_in_4bit=True, 
    bnb_4bit_compute_dtype=torch.bfloat16, 
    bnb_4bit_use_double_quant=True, 
    bnb_4bit_quant_type="nf4",
)
llama_model = load_and_quantize_model(
    empty_model, 
    weights_location=llama_path, 
    bnb_quantization_config=bnb_quantization_config, 
    device_map = "auto",
)
llama_tokenizer.padding_side = 'left'
llama_tokenizer.pad_token = llama_tokenizer.unk_token

# Load SQuAD2.0 dataset
dataset = load_dataset("squad_v2")
val_dataset = dataset['validation']

# Group validation by context
df = pd.DataFrame(val_dataset)
grouped_df = df.groupby("context").agg(list)
grouped_df.reset_index(inplace=True)
val_dataset_grouped = Dataset.from_pandas(grouped_df)

if DEBUG:
    val_dataset_grouped = val_dataset_grouped.select(range(25))
    
# In-context examples
context_1 = "Context:\nA self-described \"modern-day feminist\", Beyoncé creates songs that are often characterized by themes of love, relationships, and monogamy, as well as female sexuality and empowerment. On stage, her dynamic, highly choreographed performances have led to critics hailing her as one of the best entertainers in contemporary popular music. Throughout a career spanning 19 years, she has sold over 118 million records as a solo artist, and a further 60 million with Destiny's Child, making her one of the best-selling music artists of all time. She has won 20 Grammy Awards and is the most nominated woman in the award's history. The Recording Industry Association of America recognized her as the Top Certified Artist in America during the 2000s decade. In 2009, Billboard named her the Top Radio Songs Artist of the Decade, the Top Female Artist of the 2000s and their Artist of the Millennium in 2011. Time listed her among the 100 most influential people in the world in 2013 and 2014. Forbes magazine also listed her as the most powerful female musician of 2015.\n"
summary_1 = "Summary:\nBeyoncé, a modern-day feminist and acclaimed entertainer, has achieved remarkable success as a best-selling artist with numerous awards, including 20 Grammys, and is recognized for her empowering themes, dynamic performances, and significant influence in music and beyond.\n"
quiz_1 = "Question:\nHow many Grammy awards has Beyoncé won?\n"
icl_example_1 = context_1 + summary_1 + quiz_1 + "\n"
control_example_1 = context_1 + quiz_1 + "\n"

context_2 = "Context:\nOn 16 March 1934, President Franklin D. Roosevelt signed the Migratory Bird Hunting Stamp Act, which requires an annual stamp purchase by all hunters over the age of sixteen. The stamps are created on behalf of the program by the US Postal Service and depict wildlife artwork chosen through an annual contest. They play an important role in habitat conservation because ninety-eight percent of all funds generated by their sale go directly toward the purchase or lease of wetland habitat for protection in the National Wildlife Refuge System.[citation needed] In addition to waterfowl, it is estimated that one third of the nation's endangered species seek food and shelter in areas protected using Duck Stamp funds.[citation needed]\n"
summary_2 = "Summary:\nThe 1934 Migratory Bird Hunting Stamp Act, signed by President Roosevelt, requires hunters to buy annual stamps, with proceeds largely funding wildlife habitat conservation and benefiting waterfowl and endangered species.\n"
quiz_2 = "Question:\nWhat act was signed in 1934?\n"
icl_example_2 = context_2 + summary_2 + quiz_2 + "\n"
control_example_2 = context_2 + quiz_2 + "\n"

icl_examples = [icl_example_1, icl_example_2]
control_examples = [control_example_1, control_example_2]

# Prepare input to the model
def generate_batched_prompt(batch):
    text = [data["context"] for data in batch]
    if CONTROL:
        icl_set = control_examples[:NUM_SHOTS]
    else:
        icl_set = icl_examples[:NUM_SHOTS]
    icl_example = "".join(icl_set)
    prompts = [icl_example + "Context:\n" + t + "\n" for t in text]
    inputs = llama_tokenizer(
        prompts,
        padding=True, 
        return_tensors="pt"
    )
    return {
        "text": text,
        "prompts": prompts,
        "questions": [data["question"] for data in batch], # list[list[str]] | list[str]
        **inputs
    }

# Evaluation helper
rouge = load_metric("rouge")
def compute_metrics(pred: list[str], labels: list[str]):
    rouge_output = rouge.compute(
        predictions=pred, references=labels, rouge_types=["rouge2"]
    )["rouge2"].mid

    return {
        "rouge2_precision": round(rouge_output.precision, 4),
        "rouge2_recall": round(rouge_output.recall, 4),
        "rouge2_fmeasure": round(rouge_output.fmeasure, 4),
    }

val_grouped_dataloader = DataLoader(val_dataset_grouped, batch_size=BATCH_SIZE, shuffle=False, collate_fn=generate_batched_prompt)

wandb.init(
    project="EECS487FinalProj",
    config={
        "batch_size": BATCH_SIZE,
        "num_shots": NUM_SHOTS,
    }
)
table = wandb.Table(
    columns=[
        "pred",
        "best_match",
        "rouge2_score",
        "ground_truth",
        "prompt",
        "context",
        "generation",
    ]
)

for dp in tqdm(val_grouped_dataloader):
    if DEBUG:
        print("Generating summary...")
        print(f'Number of input tokens: {len(dp["input_ids"][0])}')
        print("Prompt:")
        print(dp["prompts"][0] + "\n")
    generated = llama_model.generate(
        input_ids=dp["input_ids"].to(llama_model.device), 
        attention_mask=dp["attention_mask"].to(llama_model.device),
        max_new_tokens=128,
    )
    results = llama_tokenizer.batch_decode(generated[:, len(dp["input_ids"][0]):])
    if DEBUG:
        print("Generated:")
        print(results[0] + "\n")
    
    questions = [re.search("Question:\n(.+)", r).groups()[0] if re.search("Question:\n(.+)", r) is not None else None for r in results]
    if DEBUG:
        print("Generated question:")
        print(questions[0])
        print("Ground truth:")
        print(dp["questions"][0])
    
    for pred, trues, prompt, context, generation in zip(questions, dp["questions"], dp["prompts"], dp["text"],results):
        if pred is None:
            continue
        best_score = {
            "rouge2_precision": -1,
            "rouge2_recall": -1,
            "rouge2_fmeasure": -1,
        }
        match = ""
        for true in trues:
            score = compute_metrics([pred], [true])
            if score["rouge2_fmeasure"] > best_score["rouge2_fmeasure"]:
                best_score = score
                match = true
        if DEBUG:
            print("Best score:")
            print(best_score)
        table.add_data(
            pred,
            match,
            best_score,
            trues,
            prompt,
            context,
            generation,
        )

wandb.log({"generated": table})
        